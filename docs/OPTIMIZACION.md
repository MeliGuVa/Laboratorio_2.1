# Reglas de Oro para Optimizaci√≥n en Spark

Este documento presenta las mejores pr√°cticas identificadas durante el laboratorio para mejorar el rendimiento en Apache Spark.

## ‚úÖ Persistencia Estrat√©gica

- Usar `persist(StorageLevel.MEMORY_AND_DISK)` en transformaciones costosas.
- Evitar `cache()` si los datos no caben completamente en memoria.

## ‚úÖ Particionamiento Inteligente

- Aumentar las particiones: `spark.sql.shuffle.partitions = 2-4 * n√∫m. de n√∫cleos`
- Usar `.repartition(columna)` en lugar de `.coalesce()` para cargas balanceadas.

## ‚úÖ Formatos de Salida Eficientes

- Preferir formatos columnares: `Parquet`, `ORC` (mejor compresi√≥n y escaneo).
- Usar `.write.parquet()` en lugar de `.csv` para outputs grandes.

## ‚úÖ Optimizaci√≥n SQL + Catalyst

- Filtrar antes de agrupar (`push-down filters`).
- Aprovechar vistas temporales y expresiones SQL para el motor Catalyst.

## üß† Analog√≠a √∫til

> "Los RDD son como arcilla en bruto que tienes que moldear a mano, los DataFrames son piezas de cer√°mica ya moldeadas listas para usar, y SQL es como comprar una vajilla completa y decorada, lista para la mesa."

## üõ†Ô∏è Checklist de Optimizaci√≥n

- [x] Persistencia en operaciones clave
- [x] Repartici√≥n controlada
- [x] Uso de formatos columnares
- [x] Reducci√≥n de shuffles innecesarios
- [x] Consultas SQL optimizadas
